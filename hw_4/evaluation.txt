Improvements in bestbayes.py:
- all words lowercase and no punctuation; so “Great!” and “great” will map to the same word in the dictionary
- got rid of prior probability, since the training corpus had many more positive reviews than negative reviews . In practice, if the ratio was not so skewed towards positive reviews, then this would have yielded better performance.

We automated the cross-validation and calculated performance metrics automatically, using a modulo approach to have each training and testing set have roughly similar counts of positive and negative reviews to train and test on, respectively.

Results after removing punctuation:

naive bayes classifier:
   precision_positive: 0.750
   precision_negative: 0.937
   recall_positive: 0.980
   recall_negative: 0.479
   f_measure_positive: 0.849
   f_measure_negative: 0.634

naive bayes classifier (improved):
   precision_positive: 0.791
   precision_negative: 0.925
   recall_positive: 0.977
   recall_negative: 0.521
   f_measure_positive: 0.874
   f_measure_negative: 0.666

Results after removing punctuation and prior-probabilities:

naive bayes classifier:
   [same as above]

naive bayes classifier (improved):
   precision_positive: 0.703
   precision_negative: 0.974
   recall_positive: 0.991
   recall_negative: 0.446
   f_measure_positive: 0.822
   f_measure_negative: 0.612